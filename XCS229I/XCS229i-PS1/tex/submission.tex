% This contents of this file will be inserted into the _Solutions version of the
% output tex document.  Here's an example:

% If assignment with subquestion (1.a) requires a written response, you will
% find the following flag within this document: <SCPD_SUBMISSION_TAG>_1a
% In this example, you would insert the LaTeX for your solution to (1.a) between
% the <SCPD_SUBMISSION_TAG>_1a flags.  If you also constrain your answer between the
% START_CODE_HERE and END_CODE_HERE flags, your LaTeX will be styled as a
% solution within the final document.

% Please do not use the '<SCPD_SUBMISSION_TAG>' character anywhere within your code.  As expected,
% that will confuse the regular expressions we use to identify your solution.
\def\assignmentnum{1 }
\def\assignmenttitle{XCS229i Problem Set \assignmentnum}
\input{macros}
\begin{document}
\pagestyle{myheadings} \markboth{}{\assignmenttitle}

% <SCPD_SUBMISSION_TAG>_entire_submission

This handout includes space for every question that requires a written response.
Please feel free to use it to handwrite your solutions (legibly, please).  If
you choose to typeset your solutions, the |README.md| for this assignment includes
instructions to regenerate this handout with your typeset \LaTeX{} solutions.
\ruleskip

\LARGE
1.a
\normalsize


% <SCPD_SUBMISSION_TAG>_1a
\begin{answer}
  % ### START CODE HERE ###
\LARGE
Lets start with the definition of that integral of density function is equal to 1 over the entire space:
\\
$ \int p (y; \eta)dy = 1 $
\\
Applying same to exponential family distribution and trying to find out $ a (\eta) $:
\\ 
$ \int p(y; \eta) dy = \int  b(y)\exp(\eta y - a(\eta)) dy = 1 $ i.e
\\  
$  \int  b(y)\exp(\eta y - a(\eta)) dy = 1 $
\\ We can rewrite this as: \\
$ \exp(-a(\eta)) \int b(y)\exp(\eta y) dy = 1 $
\\ 
$ \exp(a(\eta)) = \int b(y)\exp(\eta y) dy $ 
\\ 
$ a(\eta) = \log  \int b(y)\exp(\eta y) dy $ 
\\ Lets take derivative of $a(\eta)$ with respect to $\eta$ and apply hint provided in 1.a
\\ 
$ \frac {\partial a(\eta)}{ \partial \eta}  = \frac{\partial}{\partial (\eta) } \log  \int b(y)\exp(\eta y) dy $
\\
$= \frac{\int yb(y) \exp (\eta y) dy} {\int b(y) \exp (\eta y) dy} $
\\ As per pervious steps we can replace denominator with $ \exp a(\eta) $ 
\\
$=  \frac{\int yb(y) \exp (\eta y) dy} { \exp a(\eta)} $
\\
$=  \int  y b(y)\exp(\eta y - a(\eta)) dy = E [Y;\eta] $
\\ \\ This proves that the first derivate of $a(\eta$) w.r.t $\eta$ is equivalent to the mean of exponentail family distribution

  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_1a
\clearpage

\LARGE
1.b
\normalsize

% <SCPD_SUBMISSION_TAG>_1b
\begin{answer}
  % ### START CODE HERE ###
\LARGE
Lets startby computing second derivative of $a(\eta)$ w.r.t $\eta$ using defintion computed in previous answer 1.a
\\
$ \frac {\partial^2 a(\eta)}{ \partial \eta^2}  = \frac{\partial}{\partial (\eta) }   \int  y b(y)\exp(\eta y - a(\eta)) dy $
\\   \\ 
$ =  \int  y b(y)\exp(\eta y - a(\eta)) (y-a^\prime(\eta)) dy$
\\   \\ 
$ =  \int p(y;\eta)y^2dy -a^\prime(\eta) \int p(y;\eta) ydy $
\\   \\ 
$ = E[Y^2;\eta] - E[Y;\eta] E[Y;\eta] $
\\   \\ 
$ = Var[Y;\eta] $ \\ \\
This shows that the variance of an exponential family distribution is the second derivative of the log-partition function w.r.t. the natural parameter.

  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_1b
\clearpage

\LARGE
1.c
\normalsize

% <SCPD_SUBMISSION_TAG>_1c
\begin{answer}
  % ### START CODE HERE ###
  \LARGE
  Lets start with definition of negative log likelyhood
  $ NLL = -log(p(y;\eta))$ \\
  $ = -log(b(y) \exp (\eta y- a(\eta))) $ \\
  $ = -(log(b(y)) + log(\exp(\eta y- a(\eta)))) $ \\ This can be rewritten as \\
  $ = -(log(b(y)) + (\eta y- a(\eta))) $ \\
  $ = -(log(b(y)) + (\theta^{T}xy- a(\theta^{T}x))) $ \\ \\ 
  Now lets take hessian of the NLL wrt to $\theta$ : \\
  $ \nabla^{2}_\theta(NLL) = \nabla^{2}_\theta ( -(log(b(y)) + (\theta^{T}xy- a(\theta^{T}x)))  ) $ \\
  $ = \nabla^{2}_\theta ( -(\theta^{T}xy- a(\theta^{T}x))) $ \\ 
  The second order derivative of $  -(\theta^{T}xy)$ w.r.t $\theta $ is equal to 0, so:\\
  $ =  \nabla^{2}_\theta (a(\theta^{T}x)) $ \\ 
  $ = Var(Y;\eta)$ \\
  As variance of any probability distribution is non negative and therefore the Hessian of GLMâ€™s NLL loss is PSD, and hence convex.
  
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_1c
\clearpage

\LARGE
2.a
\normalsize

% <SCPD_SUBMISSION_TAG>_2a
\begin{answer}
  \begin{align*}
      J(\theta)&=\\
      % ### START CODE HERE ###
\frac{1}{2} \sum_{i=1}^{\nexp} (h_\theta(\hat{x}^{(i)} - \ysi)^2.      
      % ### END CODE HERE ###
  \end{align*}

  Differentiating this objective, we get:
  \begin{align*}
      \nabla_{\theta} J(\theta)&=\\
      % ### START CODE HERE ###
      \frac{\partial}{\partial \theta_{j}}\frac{1}{2}  (h_\theta(\hat{x}) - y)^2 = \\
      (h_\theta(\hat{x}) - y)x_{j}
      % ### END CODE HERE ###
  \end{align*}
  % ### START CODE HERE ###
  % ### END CODE HERE ###
  The gradient descent update rule is
  %
  \begin{equation*}
  \theta := \theta - \lambda \nabla_{\theta} J(\theta)
  \end{equation*}
  %
  which reduces here to:
  % ### START CODE HERE ###
  \\
$  \theta := \theta - \lambda (h_\theta(\hat{x}) - y)x_{j} $
\\ Rearranging terms and in general for i \\
$  \theta := \theta + \lambda (y^{(i)} - h_\theta(\hat{x}^{(i)}))\hat{x}_{j}^{(i)} $
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_2a
\clearpage

\LARGE
2.d
\normalsize

% <SCPD_SUBMISSION_TAG>_2d
\begin{answer}
  % ### START CODE HERE ###
  \Large
  For k=1 (or 2) the fit is almost a straight line\\
  For k=3 the fit starts to show the sin wave pattern \\
  For k=5,10 the fit is more natural to data points and is closer also\\
  For k=20 the curve passes through most of the points and also start showing signs of overfitting as we can see some curvatures beyond the given point.\\
So as k increases fit is passing through more and more points and tends to overfitting.
  
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_2d
\clearpage

\LARGE
2.f
\normalsize

% <SCPD_SUBMISSION_TAG>_2f
\begin{answer}
  % ### START CODE HERE ###
  \Large
  Compared to 2.c  we can see the fitted model taking a sin wave pattern. \\
  This is even true for low values of k like 1 or 2. \\
  The reason for this is that the training data is also created using sin function. \\
  After adding a sin(x) to the polynomial regression even for low value of x we get good fit as compared to 2.c \\
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_2f
\clearpage

\LARGE
2.h
\normalsize

% <SCPD_SUBMISSION_TAG>_2h
\begin{answer}
  % ### START CODE HERE ###
  \Large
  As the training dataset is small the fitting of the training dataset changes with K as follows:\\ \\ 
  For polynomial regression, \\
  For lower values of K (= 1 or 2) the fit is not going over any data point\\
  For K (=3,5) fit is closer to training data point or passes through some of the training data points and is more natural \\
  But as K increases(10,20) we can see overfitting i.e the long curves in sin wave\\  \\  
  For polynomial and sinusoidal features, \\ 
  Fitting of the data is more natural even with lower values of K like 1,2,3,5 \\
  But as K increases we can see overfitting
  % ### END CODE HERE ###
\end{answer}
% <SCPD_SUBMISSION_TAG>_2h
\clearpage

% <SCPD_SUBMISSION_TAG>_entire_submission

\end{document}
