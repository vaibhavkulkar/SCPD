\documentclass{article}
%\documentstyle[11pt,handout,psfig]{article}

\usepackage{fullpage,amssymb,amsmath,epsf}
\usepackage[12pt]{extsizes}

%These give really tight margins:
%\setlength{\topmargin}{-0.3in}
%\setlength{\textheight}{8.10in}
%\setlength{\textwidth}{5.8in}           
%\setlength{\baselineskip}{0.1875in}     
%\addtolength{\leftmargin}{-2.775in}
%\setlength{\footskip}{0.45in}
%\setlength{\oddsidemargin}{0.5in}
%\setlength{\evensidemargin}{0.5in}
%%\setlength{\headsep}{0pt}
%%\setlength{\headheight}{0pt}

%\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{8in}
%\setlength{\textwidth}{5.0in}           
%\setlength{\baselineskip}{0.1875in}     
%\addtolength{\leftmargin}{-2.775in}
%\setlength{\footskip}{0.45in}
%\setlength{\oddsidemargin}{0.5in}
%\setlength{\evensidemargin}{0.5in}
%%\setlength{\headsep}{0pt}
%%\setlength{\headheight}{0pt}


\markright{CS229 Winter 2003}
\pagestyle{myheadings}

\input{macros}
\begin{document}
\title{CS229 Lecture notes}
\author{Andrew Ng}
\date{Lectures from 11/2/04 to 11/4/04}
\maketitle


\noindent
%Note: These notes are preliminary, and will be updated after we finish 
%talking about model selection in class.


\setcounter{part}{5}
\part{Regularization and model selection}

%\begin{center}
%\epsfxsize=3in
%\epsffile{foo.eps}
%\end{center}

Suppose we are trying select among several different models
for a learning problem.  For instance, we might be using a polynomial
regression model 
$h_\theta(x) = g(\theta_0 + \theta_1 x + \theta_2 x^2 + \cdots + \theta_k x^k)$,
and wish to decide if $k$ should be 0, 1, \dots, or 10.  
How can we automatically select a
model that represents a good tradeoff 
between the twin evils of bias and variance\footnote{Given that we
said in the previous set of notes that bias and variance are two very different 
beasts, some readers may be wondering if we should be calling them ``twin'' 
evils here.  Perhaps it'd be better to think of them as non-identical twins. 
The phrase ``the fraternal twin evils of bias and variance'' doesn't have 
the same ring to it, though.}?
Alternatively, suppose we want to 
automatically choose the bandwidth parameter $\tau$ for locally weighted regression, 
or the parameter $C$ for our $\ell_1$-regularized SVM.  How can we 
do that?

For the sake of concreteness, in these notes we assume we
have some finite set of models $\calM = \{M_1, \ldots, M_d\}$
that we're trying to select among.  For instance, in our first
example above, the model $M_i$ would be an 
$i$-th order polynomial regression model.
%consider the problem of choosing the degree from 1, \ldots, k=10.
(The generalization to infinite $\calM$ is not 
hard.\footnote{If we are trying to choose from an infinite set of models, say 
corresponding to the possible values of the bandwidth $\tau \in \Re^+$, we 
may discretize $\tau$ and consider only a finite number 
of possible values for it.  More generally, most of the algorithms described here 
can all be viewed as performing optimization search in the space
of models, and we can perform this search over infinite model
classes as well.}) 
Alternatively, if we are trying to decide between using an SVM,
a neural network or logistic regression, then $\calM$ may 
contain these models. 


\section{Cross validation} 

Lets suppose we are, as usual, given a training set $S$.  
Given what we know about empirical risk minimization,
%the rationale for fitting models to training data,
here's what might initially seem like a algorithm, 
resulting from using empirical risk minimization for model selection:
\begin{enumerate}
\item Train each model $M_i$ on $S$, to get some hypothesis $h_i$.
\item Pick the hypotheses with the smallest training error.
\end{enumerate}

This algorithm does \emph{not} work.  Consider choosing the order
of a polynomial.  The higher the order of the polynomial, the better
it will fit the training set $S$, and thus the lower the training error.
Hence, this method will always select a high-variance, high-degree polynomial
model, which we saw previously is often poor choice.

Here's an algorithm that works better.  In {\bf hold-out cross validation} 
(also called {\bf simple cross validation}),
we do the following:
\begin{enumerate}
\item Randomly split $S$ into $\Strain$ (say, 70\% of the data) and 
$\Scv$ (the remaining 30\%).  Here, $\Scv$ is called the hold-out
cross validation set. 
\item Train each model $M_i$ on $\Strain$ only, to get some hypothesis $h_i$.
\item Select and output the hypothesis $h_i$ that had the smallest 
   error $\ehat_{\Scv}(h_i)$ on the hold out cross validation set.  
   (Recall, $\ehat_\Scv(h)$ denotes the empirical error of $h$ on the set
   of examples in $\Scv$.) 
\end{enumerate}

By testing on a set of examples $\Scv$ that the models were not trained on,
we obtain a better estimate of each hypothesis $h_i$'s true generalization
error, and can then pick the one with the smallest estimated generalization error. 
Usually, somewhere between $1/4 - 1/3$ of the data is used in the hold out
cross validation set, and 30\% is a typical choice. 

Optionally, step 3 in the algorithm may also be replaced with selecting the model 
$M_i$ according to $\arg \min_i \ehat_{\Scv}(h_i)$, and then retraining
$M_i$ on the entire training set $S$.  (This is often a good idea,
with one exception being learning algorithms that are be very sensitive to 
perturbations of the initial conditions and/or data.  
%This arguably includes neural networks trained by gradient descent, 
%which is susceptible to local optimal and may be sensitive to 
%variations in the initial conditions.
For these methods, $M_i$ doing well on $\Strain$ does not necessarily 
mean it will also do well on $\Scv$, and it might be better to forgo 
this retraining step.) 
%This reduces the data wastage,
%though if the learning algorithm used is very sensitive to different training
%sets, this isn't always a good idea (i.e., 

The disadvantage of using hold out cross validation is that it ``wastes'' about 30\% of the data. 
Even if we were to take the optional step of retraining the model on the 
entire training set, it's still as if we're trying to find a good model for
a learning problem in which we had $0.7m$ training examples, rather than $m$
training examples, since we're testing models that were trained on only $0.7m$
examples each time.  While this is fine if data is abundant and/or cheap,
in learning problems in which data is scarce (consider a problem with $m=20$, say), 
we'd like to do something better.

Here is a method, 
called {\bf $k$-fold
cross validation}, that 
holds out less data each time:
\begin{enumerate}
\item Randomly split $S$ into $k$ disjoint subsets of $m/k$ training examples each.  Lets
call these subsets $S_1, \ldots, S_k$.
\item For each model $M_i$, we evaluate it as follows:
%we obtain an estimate of its generalizaion error as follows:
\begin{enumerate}
\item[] For $j=1, \ldots, k$
\begin{enumerate}
\item[] Train the model $M_i$ on $S_1 \cup \cdots \cup S_{j-1} \cup S_{j+1} \cup \cdots S_{k}$
(i.e., train on all the data except $S_j$)
to get some hypothesis $h_{ij}$.  
\item[] Test the hypothesis $h_{ij}$ on $S_j$, to get $\ehat_{S_j}(h_{ij})$.
\end{enumerate}
\item[] The estimated generalization error of model $M_i$ is then calculated as the average
of the $\ehat_{S_j}(h_{ij})$'s (averaged over $j$).
\end{enumerate}
%\footnote{A minor detail: When $m$ is 
%not a multiple of $k$ so that the $k$ sets $S_i$ are not all the same size, we might 
%also take a weighted average of the $\varepsilon_{S_j}(h_{ij})$'s, where the weight of 
%$\varepsilon_{S_j}(h_{ij})$ is given by $|S_j|$.}
\item Pick the model $M_i$ with the lowest estimated generalization error, and retrain
that model on the entire training set $S$.  The resulting hypothesis is then output 
as our final answer. 
\end{enumerate}

A typical choice for the number of folds to use here would be $k=10$.  While the fraction
of data held out each time is now $1/k$---much smaller than before---this procedure may
also be more computationally expensive than hold-out cross
validation, since we now need train to each model $k$ times.

While $k=10$ is a commonly used choice, in problems in which data is really scarce, 
sometimes we will use the extreme choice of $k=m$ in order to leave out as little data as possible
each time.  In this setting, we would repeatedly train on all but one of the 
training examples in $S$, and test on that held-out example.  The resulting $m=k$ errors
are then averaged together to obtain our estimate of the generalization error of a model.
This method has its own name; 
since we're holding out one training example at a time, 
this method is called {\bf leave-one-out cross validation.}

Finally, even though we have described the different versions of 
cross validation as methods for selecting a model, they can 
also be used more simply to evaluate a \emph{single} model 
or algorithm.  For example, if you have implemented some learning 
algorithm and want to estimate how well it performs for your 
application (or if you have invented a novel learning algorithm 
and want to report in a technical paper how well it performs on 
various test sets), cross validation would give a reasonable
way of doing so.

\section{Feature Selection}

One special and important case of model selection is called 
feature selection.  To motivate this, imagine that you have 
a supervised learning problem where the number of features $n$ 
is very large (perhaps $n \gg m$), but you suspect that there is
only a small number of features that are ``relevant'' to a task.  
Even if you use the a simple linear classifier (such as the 
perceptron) over the $n$ input features, the VC dimension of 
your hypothesis class would be $O(n)$, and thus overfitting 
is a potential problem the training set is fairly large.

In this, you can apply a feature selection algorithms to reduce
the size of the set of features.  Given $n$ features, 
there are $2^n$ possible feature subsets (since each of $n$
features can either be included or excluded from the subset), 
and thus this can be viewed as a model selection problem over $2^n$
possible models.  For large values of $n$, it's usually too 
expensive to explicitly enumerate over the $2^n$ different models,
and so typically some heuristic search procedure is used to find a
good feature subset.  The following search procedure is called
{\bf forward search}: 
\begin{enumerate}
\item Initialize $\calF = \emptyset$. 
\item Repeat $\{$
\begin{enumerate}
\item For $i=1,\ldots,n$ if $i \not\in \calF$, let $\calF_i = \calF \cup \{i\}$,
and use some version of cross version to evaluate the model that uses features $\calF_i$.
\item Set $\calF$ to be the best feature subset found on step (a).
\end{enumerate}
\item[] $\}$ 
\item Select and output the best feature subset that was evaluated 
during the entire search procedure. 
\end{enumerate}

The outer loop of the algorithm can be terminated either 
when $\calF = \{1, \ldots, n\}$ is 
the set of all features, or when $|\calF|$ exceeds some pre-set threshold
(corresponding to the maximum number of features that you want the algorithm
to consider).

This algorithm is one instantiation of {\bf wrapper feature selection},
since it is a procedure that ``wraps'' around your learning algorithm, 
and repeatedly makes calls to the learning algorithm to evaluate how
well it does using different feature subsets.  Other search procedures
than forward search can also be used, for example, {\bf backward search}
starts off with $\calF = \{1, \ldots, n\}$ as the set of all features,
and then repeatedly deletes them one at a time (evaluating single-feature
deletions similar to how forward search evaluates single-feature additions).

Wrapper feature selection algorithms often work quite well, but can also
be computationally expensive given how frequently it has to call the learning
algorithm. Indeed, complete forward search (terminating when $\calF = \{1,\ldots,n\}$)
would take about $O(n^2)$ calls to the learning algorithm. 

{\bf Filter feature selection} methods give a heuristic,
%\footnote{A {\bf heuristic}
%refers to any method---perhaps a rule-of-thumb---that seems to work 
%reasonably well in practice, but lacks formal guarantees.} 
but faster, way 
of choosing a feature subset.  The idea here is to compute some 
simple score $S(i)$ that measures how well informative each 
feature $x_i$ is about the class labels $y$.  Then, we simply pick the
$k$ features with the largest scores $S(i)$.  

One simple way of choosing the score would be define $S(i)$ to be (the
absolute value of) the correlation between $x_i$ and $y$ (as measured on the training data).  In
practice, it is more common (particularly for discrete-valued features $x_i$)
to choose $S(i)$ to be the {\bf mutual information} $\MI(x_i, y)$ between
$x_i$ and $y$:
\[
\MI(x_i,y) = \sum_{x_i \in \{0,1\}} \sum_{y \in \{0,1\}} p(x_i, y) \log \frac{p(x_i,y)}{p(x_i)p(y)}. 
\]
(The equation above assumes that $x_i$ and $y$ are binary-valued; more generally 
the summations would be over the domains of the variables.)  The probabilities abovbe 
$p(x_i, y)$, etc. can all be estimated according to their empirical distributions on
the training set. 

To gain intuition
about what this does, we note that the mutual information can also be expressed as a KL-divergence:
\[
\MI(x_i,y) = KL\left(p(x_i,y) || p(x_i) p(y)\right)
\]
You'll get to play more with KL-divergence in Problem set \#3, but intuitively, 
this gives a measure of how different are the probability 
distributions 
$p(x_i,y)$ and $p(x_i) p(y)$.  If $x_i$ and $y$ are independent random variables,
then we would have $p(x_i,y) - p(x_i) p(y)$, and the KL-divergence between the 
two distributions will be zero.  This is consistent with the idea if $x_i$ and $y$
are independent, then $x_i$ is clearly very ``non-informative'' about $y$, and thus
the score $S(i)$ should be small.  Conversely, if $x_i$ is very ``informative''
about $y$, then $\MI(x_i,y)$ would be large. 

One final detail: Now that you've ranked the features according to their scores $S(i)$,
how do you decide how many features $k$ to choose?  Well, one standard way to 
do so is to use cross validation to select among the possible values of $k$.  


\section{Bayesian statistics and regularization}

Apart from model selection, we still have another arsenal of tools in our battle
against overfitting. 

At the beginning of the quarter, we talked about model fitting using maximum likelihood, 
where we would choose the parameters according to 
\[
\max_\theta \prod_{i=1}^m p(y^{(i)} | x^{(i)};\theta).
\]
Throughout our discussions, we viewed $\theta$ as an unknown
parameter of the world.
This view of the $\theta$ as being \emph{constant-valued
but unknown} is taken in {\bf frequentist} statistics.  
Note that in frequentist this view of the world, $\theta$ is 
not random---it just happens to be unknown---and it's our job to come 
up with statistical procedures (such as maximum likelihood) to try to 
estimate this parameter.  

An alternative way to approach these estimation problems is to take the {\bf Bayesian}
view of the world, and think of $\theta$ as being an unknown \emph{random variable}.
In this approach, we would specify a {\bf prior distribution} $p(\theta)$ on $\theta$
that expresses about our ``prior beliefs'' about the parameters.  Given a training set 
$S = \{(x^{(i)}, y^{(i)})\}_{i=1}^m$, when we are asked to make a prediction on a 
new value of $x$, we can then compute the posterior distribution on the parameters
\begin{eqnarray}
p(\theta | S) &=& \frac{p(S|\theta) p(\theta)}{p(S)}  \nonumber \\
  &=& 
  \frac{\prod_{i=1}^m p(\ysi | \xsi, \theta) p(\theta)}{\int_\theta \prod_{i=1}^m p(\ysi | \xsi, \theta) p(\theta) d\theta} \label{eqn-posterior}
\end{eqnarray}
In the equation above, $p(\ysi|\xsi,\theta)$ comes from whatever model you're
using for your learning problem.  For example, if you are using Bayesian logistic
regression, then you might choose $p(\ysi | \xsi,\theta) = 
h_\theta(\xsi)^\ysi
(1-h_\theta(\xsi))^{(1-\ysi)}$, where 
$h_\theta(\xsi) = 1/(1+\exp(-\theta^T\xsi))$.
Next, given the new test example $x$, we can compute our posterior 
distribution on the class label using the 
posterior distribution on $\theta$:
\begin{equation}
p(y|x, S) = \int_\theta p(y|x,\theta) p(\theta|S) d\theta  \label{eqn-avgy}
\end{equation}
In the equation above, $p(\theta|S)$ comes from Equation~(\ref{eqn-posterior}).  
If the goal is the predict the expected value of $y$ given $x$, then we would
predict\footnote{The integral below would be replaced by a summation if $y$ is discrete-valued.}
\[
\E[y|x,S] = \int_y y p(y|x, S) dy
\]

The procedure that we've outlined here can be thought of as the ``fully Bayesian prediction,''
where we are essentially averaging our predictions with respect to the posterior distribution over 
$\theta$.  Unfortunately, in general it is computationally very difficult to compute these
posterior distribution.  This is because it requires taking 
integrals over the (usually high-dimensional) $\theta$, which typically cannot be 
computed in closed-form.  Thus, the posterior distribution for $\theta$ has to be approximated.

One common approximation is to replace our posterior distribution for $\theta$
(as in Equation~\ref{eqn-avgy}) with a single point estimate.  The MAP (maximum a posteriori)
estimate for $\theta$ is given by 
\begin{equation}
\theta_{\rm MAP} \max_\theta \prod_{i=1}^m p(\ysi | \xsi, \theta) p(\theta).
\end{equation}
Note that this is the same formulas as for the ML (maximum likelihood) estimate for $\theta$,
except for the prior $p(\theta)$ term at the end.  

In many applications, a common choice for the prior $p(\theta)$ is to assume that
$\theta \sim \calN(0, \tau^2 I)$.   Using this choice of prior, the fitted parameters
$\theta_{\rm MAP}$ will have smaller norm than that selected by maximum likelihood.
(See Problem set \#3.)  
% The parameter $\tau^2$ controls the ``strength'' of the prior,
%and for large values of $\tau$, $\theta_{\rm MAP}$ will become i
In practice, this causes the Bayesian MAP estimate to be less
susceptible to overfitting than the ML estimate of the parameters.  For example,
Bayesian logistic regression turns out to be a highly effective algorithm for 
text classification. 

\end{document}

