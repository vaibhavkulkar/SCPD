\documentclass{article}
%\documentstyle[11pt,handout,psfig]{article}

\usepackage{fullpage,amssymb,amsmath,epsf}
\usepackage[12pt]{extsizes}

%These give really tight margins:
%\setlength{\topmargin}{-0.3in}
%\setlength{\textheight}{8.10in}
%\setlength{\textwidth}{5.8in}           
%\setlength{\baselineskip}{0.1875in}     
%\addtolength{\leftmargin}{-2.775in}
%\setlength{\footskip}{0.45in}
%\setlength{\oddsidemargin}{0.5in}
%\setlength{\evensidemargin}{0.5in}
%%\setlength{\headsep}{0pt}
%%\setlength{\headheight}{0pt}

%\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{8in}
%\setlength{\textwidth}{5.0in}           
%\setlength{\baselineskip}{0.1875in}     
%\addtolength{\leftmargin}{-2.775in}
%\setlength{\footskip}{0.45in}
%\setlength{\oddsidemargin}{0.5in}
%\setlength{\evensidemargin}{0.5in}
%%\setlength{\headsep}{0pt}
%%\setlength{\headheight}{0pt}


%\markright{CS229 Winter 2003}
\pagestyle{myheadings}

\input{macros}
\input{notations_macros}
\begin{document}
\title{XCS229i Lecture Notes}
\author{Andrew Ng}
\date{}
\maketitle



%\setcounter{part}{5}
%\part{Regularization and model selection}

\section*{The $k$-means clustering algorithm} 

In the clustering problem, we are given a training set $\{x^{(1)}, \ldots, x^{(\nexp)}\}$,
and want to group the data into a few cohesive ``clusters.''  
Here, $x^{(i)} \in \Re^\di$ as usual; but 
no labels $y^{(i)}$ are given. So, this is an unsupervised learning problem.

The $k$-means clustering algorithm is as follows:
\begin{itemize}
\item[1.] Initialize {\bf cluster centroids} $\mu_1, \mu_2, \ldots, \mu_k \in \Re^\di$ randomly.
\item[2.] Repeat until convergence: $\{$
\begin{itemize}
\item[] For every $i$, set 
\[
c^{(i)} := \arg \min_j ||x^{(i)} - \mu_j||^2.
\]
\item[] For each $j$, set 
\[
\mu_j := \frac{\sum_{i=1}^\nexp 1\{c^{(i)}=j\} x^{(i)}}{\sum_{i=1}^\nexp 1\{c^{(i)}=j\}}. 
\]
\item[] $\}$
\end{itemize}
\end{itemize}

In the algorithm above, $k$ (a parameter of the algorithm) is the number of
clusters we want to find; and the cluster centroids $\mu_j$ represent our
current guesses for the positions of the centers of the clusters.  To initialize the
cluster centroids (in step 1 of the algorithm above), we could choose $k$ training 
examples randomly, and set the cluster centroids to be equal to the values of 
these $k$ examples.  (Other initialization methods are also possible.) 

The inner-loop of the algorithm repeatedly carries out two steps: (i)
``Assigning'' each training example $\xsi$ to the closest cluster centroid
$\mu_j$, and (ii) Moving each cluster centroid $\mu_j$ to the mean of the
points assigned to it.  Figure below shows an illustration of
running $k$-means.

% \begin{figure}[t]
% Using eps library outdated
% \threefigbox{kmeans/kmeans1.eps}{kmeans/kmeans2.eps}{kmeans/kmeans3.eps} 

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{kmeans/kmeans1.eps}
    \caption{}
    \label{}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{kmeans/kmeans2.eps}
    \caption{}
    \label{}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{kmeans/kmeans3.eps}
    \caption{}
    \label{}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{kmeans/kmeans4.eps}
    \caption{}
    \label{}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{kmeans/kmeans5.eps}
    \caption{}
    \label{}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{kmeans/kmeans6.eps}
    \caption{}
    \label{}
  \end{subfigure}
  \caption{K-means algorithm. Training examples are shown as dots, and cluster
  centroids are shown as crosses.  (a) Original dataset. (b) Random initial cluster
  centroids (in this instance, not chosen to be equal to two training examples). (c-f) Illustration of running
  two iterations of $k$-means.  In each iteration, we assign each training example 
  to the closest cluster centroid (shown by ``painting'' the training examples 
  the same color as the cluster centroid to which is assigned); then we move
  each cluster centroid to the mean of the points assigned to it.  (Best viewed in color.) 
  Images courtesy Michael Jordan.}
  \label{}
\end{figure}

% \begin{figure}[t]

%   \centering
%   \subfigure[]{\includegraphics[width=0.24\textwidth]{kmeans/kmeans4.eps}}
%   \subfigure[]{\includegraphics[width=0.24\textwidth]{kmeans/kmeans5.eps}} 
%   \subfigure[]{\includegraphics[width=0.24\textwidth]{kmeans/kmeans6.eps}}
%   % \includegraphics[width=.3\textwidth]{kmeans/kmeans1.eps}\hfill
%   % \includegraphics[width=.3\textwidth]{kmeans/kmeans2.eps}\hfill
%   % \includegraphics[width=.3\textwidth]{kmeans/kmeans3.eps}
%   \caption{(d) (e) (f)}
%   \label{fig-kmeans-2}
%   % \caption{default}
%   % \label{fig:figure3}
% \end{figure}


% \vskip 0.1in
% \threefigboxdef{kmeans/kmeans4.eps}{kmeans/kmeans5.eps}{kmeans/kmeans6.eps} 
% \end{figure}

Is the $k$-means algorithm guaranteed to converge?  Yes it is, in a certain sense.
In particular, let us define the {\bf distortion function} to be: 
\[
J(c, \mu) = \sum_{i=1}^\nexp ||x^{(i)} - \mu_{c^{(i)}}||^2
\]
Thus, $J$ measures the sum of squared distances between each training example $\xsi$
and the cluster centroid $\mu_{c^{(i)}}$ to which it has been assigned.  It can
be shown that $k$-means is exactly coordinate descent on $J$.  Specifically, the inner-loop
of $k$-means repeatedly minimizes $J$ with respect to $c$ while holding $\mu$ fixed, and then
minimizes $J$ with respect to $\mu$ while holding $c$ fixed.
Thus, $J$ must
monotonically decrease, and the value of $J$ must converge.  (Usually, this implies
that $c$ and $\mu$ will converge too.  In theory, it is possible
for $k$-means to oscillate between a few different clusterings---i.e., a few different
values for $c$ and/or $\mu$---that have exactly
the same value of $J$, but this almost never happens in practice.)

The distortion function $J$ is a non-convex function, and so coordinate 
descent on $J$ is not guaranteed to converge to the global minimum.  In other words, $k$-means
can be susceptible to local optima.  Very often $k$-means will work fine and come up with
very good clusterings despite this.  But if you are worried about getting stuck in bad 
local minima, one common thing to do is run $k$-means many times (using different
random initial values for the cluster centroids $\mu_j$).  Then, out of all the
different clusterings found, pick the one that gives the lowest distortion $J(c,\mu)$. 

\end{document}


