\documentclass{article}
%\documentstyle[11pt,handout,psfig]{article}

\usepackage{fullpage,amssymb,amsmath,epsf}
\usepackage[12pt]{extsizes}

%These give really tight margins:
%\setlength{\topmargin}{-0.3in}
%\setlength{\textheight}{8.10in}
%\setlength{\textwidth}{5.8in}
%\setlength{\baselineskip}{0.1875in}
%\addtolength{\leftmargin}{-2.775in}
%\setlength{\footskip}{0.45in}
%\setlength{\oddsidemargin}{0.5in}
%\setlength{\evensidemargin}{0.5in}
%%\setlength{\headsep}{0pt}
%%\setlength{\headheight}{0pt}

%\setlength{\topmargin}{-0.5in}
\setlength{\textheight}{8in}
%\setlength{\textwidth}{5.0in}
%\setlength{\baselineskip}{0.1875in}
%\addtolength{\leftmargin}{-2.775in}
%\setlength{\footskip}{0.45in}
%\setlength{\oddsidemargin}{0.5in}
%\setlength{\evensidemargin}{0.5in}
%%\setlength{\headsep}{0pt}
%%\setlength{\headheight}{0pt}


%\markright{CS229 Winter 2003}
\pagestyle{myheadings}

\input{macros}
\input{notations_macros}
\begin{document}
\title{XCS229i}
\author{Andrew Ng}
\date{}
\maketitle



%\setcounter{part}{5}
%\part{Regularization and model selection}

\section*{Mixtures of Gaussians and the EM algorithm}

In this set of notes, we discuss the EM (Expectation-Maximization) algorithm
for density estimation.

Suppose that we are given a training set $\{x^{(1)}, \ldots, x^{(\nexp)}\}$
as usual.  Since we are in the unsupervised learning setting, these
points do not come with any labels.

We wish to model the data by specifying a joint
distribution $p(\xsi, \zsi) = p(\xsi|\zsi) p(\zsi)$.
Here, $\zsi \sim \Multinomial(\phi)$ (where $\phi_j \geq 0$, $\sum_{j=1}^k \phi_j=1$,
and the parameter $\phi_j$ gives $p(\zsi = j)$),
and $\xsi | \zsi=j \sim \calN(\mu_j, \Sigma_j)$.  We let $k$ denote the
number of values that the $\zsi$'s can take on.  Thus, our model posits
that each $\xsi$ was generated by randomly choosing $\zsi$ from
$\{1, \ldots, k\}$, and then $\xsi$ was drawn from one of $k$ Gaussians
depending on $\zsi$.
This is called the {\bf mixture of Gaussians} model.
Also, note that the $\zsi$'s are {\bf latent} random variables,
meaning that they're hidden/unobserved.  This is what will make our
estimation problem difficult.

The parameters of our model are thus $\phi$, $\mu$ and $\Sigma$.  To
estimate them, we can write down the likelihood of our data:
\begin{eqnarray*}
\ell(\phi, \mu, \Sigma) &=& \sum_{i=1}^\nexp \log p(\xsi; \phi, \mu, \Sigma) \\
&=& \sum_{i=1}^\nexp \log \sum_{\zsi=1}^k p(\xsi| \zsi; \mu, \Sigma) p(\zsi;\phi).
\end{eqnarray*}
However, if we set to zero the derivatives of this formula with respect to the
parameters and try to solve, we'll find that it is not
possible to find the maximum likelihood estimates of the parameters
in closed form.  (Try this yourself at home.)

The random variables $\zsi$ indicate which of the $k$ Gaussians each
$\xsi$ had come from.  Note that if we knew what the $\zsi$'s were,
the maximum likelihood problem would have been easy.  Specifically,
we could then write down the likelihood as
\[
\ell(\phi, \mu, \Sigma) =
\sum_{i=1}^\nexp \log p(\xsi| \zsi; \mu, \Sigma) + \log p(\zsi;\phi).
\]
Maximizing this with respect to $\phi$, $\mu$ and $\Sigma$ gives the parameters:
\begin{eqnarray*}
\phi_j &=& \frac{1}{\nexp} \sum_{i=1}^\nexp 1\{\zsi = j\}, \\
\mu_j &=& \frac{\sum_{i=1}^\nexp 1\{\zsi = j\}\xsi}{\sum_{i=1}^\nexp 1\{\zsi = j\}}, \\
\Sigma_j &=& \frac{\sum_{i=1}^\nexp 1\{\zsi = j\} (\xsi - \mu_j) (\xsi - \mu_j)^T}{\sum_{i=1}^\nexp 1\{\zsi = j\}}.
\end{eqnarray*}

Indeed, we see that if the $\zsi$'s were known, then maximum
likelihood estimation
becomes nearly identical to what we had when estimating the parameters
of the Gaussian discriminant analysis model, except that here the $\zsi$'s
playing
the role of the class labels.\footnote{There are other minor differences in
the formulas here from what we'd obtained in PS1 with Gaussian
discriminant analysis, first because we've generalized the $\zsi$'s to
be multinomial rather than Bernoulli, and second because here we are
using a different $\Sigma_j$ for each Gaussian.}

However, in our density estimation problem, the $\zsi$'s are \emph{not}
known.  What can we do?

The EM algorithm is an iterative algorithm that has two main steps.
Applied to our problem, in
the E-step, it tries to ``guess'' the values of the $\zsi$'s.
In the M-step, it updates the parameters of our model based on
our guesses.  Since in the M-step we are pretending that the guesses
in the first part were correct, the maximization becomes easy.
Here's the algorithm:

\begin{itemize}
\item[] Repeat until convergence: $\{$
\begin{itemize}
\item[] (E-step) For each $i, j$, set
\[
w^{(i)}_j := p(\zsi = j | \xsi ; \phi, \mu, \Sigma)
\]
\item[] (M-step) Update the parameters:
\begin{eqnarray*}
\phi_j &:=& \frac{1}{\nexp} \sum_{i=1}^\nexp w^{(i)}_j, \\
\mu_j &:=& \frac{\sum_{i=1}^\nexp w^{(i)}_j \xsi}{\sum_{i=1}^\nexp w^{(i)}_j }, \\
\Sigma_j &:=& \frac{\sum_{i=1}^\nexp w^{(i)}_j (\xsi - \mu_j) (\xsi - \mu_j)^T}{\sum_{i=1}^\nexp w^{(i)}_j }
\end{eqnarray*}
\end{itemize}
\item[] $\}$
\end{itemize}

In the E-step, we calculate the posterior probability of our parameters
the $\zsi$'s, given the $\xsi$ and using the current setting of our
parameters.  I.e., using Bayes rule, we obtain:
\[
p(\zsi = j | \xsi ; \phi, \mu, \Sigma) =
\frac{p(\xsi | \zsi = j ; \mu, \Sigma) p(\zsi = j; \phi)}
{\sum_{l=1}^k p(\xsi | \zsi = l ; \mu, \Sigma) p(\zsi=l; \phi)}
\]
Here, $p(\xsi | \zsi = j ; \mu, \Sigma)$ is given by evaluating
the density of a Gaussian with mean $\mu_j$ and covariance $\Sigma_j$ at
$\xsi$; $p(\zsi = j; \phi)$ is given by $\phi_j$, and so on.
The values $w^{(i)}_j$ calculated in the E-step represent our
``soft'' guesses\footnote{The term ``soft'' refers to our guesses
being probabilities and taking values in $[0,1]$; in contrast,
a ``hard'' guess is one that represents a single best guess (such
as taking values in $\{0,1\}$ or $\{1,\ldots,k\}$).} for the
values of $\zsi$.

Also, you should contrast the updates in the M-step with the formulas we
had when the $\zsi$'s were known exactly.  They are identical,
except that instead of the indicator functions ``$1\{\zsi = j\}$''
indicating from which Gaussian each datapoint had come, we
now instead have the $w^{(i)}_j$'s.

The EM-algorithm is also reminiscent of the K-means clustering
algorithm, except that instead of the ``hard'' cluster assignments
$c(i)$, we instead have the ``soft'' assignments $w^{(i)}_j$.  Similar
to K-means, it is also susceptible to local optima, so reinitializing
at several different initial parameters may be a good idea.

It's clear that the EM algorithm has a very natural interpretation
of repeatedly trying to guess the unknown $\zsi$'s; but how did it
come about, and can we make any guarantees about it, such as regarding
its convergence?  In the next set of notes, we will describe a more
general view of EM, one that will allow us to easily apply it to other
estimation problems in which there are also latent variables, and which
will allow us to give a convergence guarantee.












%\begin{center}
%\epsfxsize=3in
%\epsffile{foo.eps}
%\end{center}

\end{document}


